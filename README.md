# ğŸ“„ Invoice Intelligence Pipeline (ETL com LLM)

Pipeline de Engenharia de Dados para extraÃ§Ã£o estruturada de informaÃ§Ãµes de Notas Fiscais (PDFs e Imagens) utilizando InteligÃªncia Artificial Generativa.

O projeto transforma documentos nÃ£o estruturados em dados validados (JSON) e os persiste automaticamente em um banco de dados relacional (PostgreSQL).

---

## ğŸš€ Funcionalidades

*   **IngestÃ£o Universal:** Aceita PDFs de qualquer layout (nÃ£o usa templates fixos ou Regex).
*   **VisÃ£o Computacional:** Converte pÃ¡ginas em imagens de alta resoluÃ§Ã£o para que a IA "veja" o documento como um humano.
*   **ExtraÃ§Ã£o SemÃ¢ntica:** Usa LLMs para entender e extrair campos especÃ­ficos (Emissor, DestinatÃ¡rio, Itens, Totais).
*   **ValidaÃ§Ã£o Rigorosa:** Utiliza **Pydantic** para garantir que os dados de saÃ­da obedeÃ§am a um contrato de dados estrito.
*   **Data Lake (Storage):** Salva os arquivos JSON originais (Raw Data) no **AWS S3** para auditoria e reprocessamento.
*   **PersistÃªncia SQL:** Salva os dados estruturados em um banco de dados PostgreSQL (via Supabase) usando **SQLAlchemy** (ORM).
*   **Processamento Incremental:** MantÃ©m histÃ³rico de arquivos pra nÃ£o reprocessar.

---

## âš ï¸ Aviso CrÃ­tico: Privacidade de Dados e LGPD

Este projeto Ã© um **LaboratÃ³rio de Engenharia**. 

Para ambientes de **ProduÃ§Ã£o** que lidam com dados reais de terceiros (CPF, CNPJ, EndereÃ§os), Ã© **IMPERATIVO** adotar uma das seguintes estratÃ©gias de seguranÃ§a:

1.  **Cloud Privada (Enterprise):** Utilizar instÃ¢ncias privadas de modelos (Azure OpenAI Service, AWS Bedrock) com contratos de "Zero Data Retention" (os dados nÃ£o sÃ£o usados para treinar os modelos pÃºblicos).
2.  **Modelos Locais (On-Premise):** Executar modelos Open Source (como Llama 3, Phi-3 ou Mistral) dentro da sua prÃ³pria infraestrutura, garantindo que os dados nunca saiam do seu servidor.

> **Nunca envie dados sensÃ­veis (PII) para APIs pÃºblicas de LLMs sem anonimizaÃ§Ã£o prÃ©via ou contratos enterprise adequados.**

---

## ğŸ› ï¸ Stack TecnolÃ³gico

*   **Linguagem:** Python 3.11+
*   **OrquestraÃ§Ã£o & LLM:** LangChain
*   **ValidaÃ§Ã£o de Dados:** Pydantic
*   **Processamento de PDF:** PyMuPDF (Fitz)
*   **Storage (Data Lake):** AWS S3 (via Boto3)
*   **Banco de Dados:** PostgreSQL
*   **ORM:** SQLAlchemy
*   **Gerenciador de Pacotes:** uv (mas compatÃ­vel com pip/poetry)

---

## ğŸ“¦ Estrutura do Projeto

```bash
/
â”œâ”€â”€ main.py              # Ponto de entrada (Entrypoint)
â”œâ”€â”€ data/                # Coloque seus PDFs aqui (Input)
â”œâ”€â”€ database/            # ConfiguraÃ§Ã£o de conexÃ£o DB
â”œâ”€â”€ schemas/             # Contratos de Dados (Pydantic Models)
â””â”€â”€ src/
    â”œâ”€â”€ extractor.py     # Orquestrador do Pipeline
    â”œâ”€â”€ llm/             # Cliente de IA
    â”œâ”€â”€ models/          # Tabelas do Banco (SQLAlchemy)
    â””â”€â”€ services/        # Regras de NegÃ³cio (Salvar no Banco e S3)
```

---

## â–¶ï¸ Como Rodar

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone https://github.com/jonhnatta/invoice-ia-pipeline.git
    cd invoice-ia-pipeline
    ```

2.  **Configure o Ambiente:**
    Crie um arquivo `.env` na raiz:
    ```ini
    OPENAI_API_KEY=sk-...
    DATABASE_URL=postgresql://user:pass@host:5432/postgres
    
    # AWS (Para S3)
    AWS_ACCESS_KEY_ID=...
    AWS_SECRET_ACCESS_KEY=...
    AWS_REGION=us-east-1
    AWS_S3_BUCKET=nome-do-seu-bucket
    ```

3.  **Instale as dependÃªncias:**
    ```bash
    # Se usar uv:
    uv sync
    ```

4.  **Execute o Pipeline:**
    Coloque seus PDFs na pasta `data/` e rode:
    ```bash
    uv run main.py
    # ou
    python main.py
    ```

O sistema irÃ¡ processar os arquivos novos, gerar os JSONs na pasta `output/` e inserir os registros no seu banco de dados automaticamente.

---
**Desenvolvido como case de estudo para Engenharia de Dados com IA.**
